# from catalyst.data.dataset import DatasetFromSampler
from collections import defaultdict, deque
from glob import glob
# from operator import itemgetter
from torch.utils.data import Sampler, DistributedSampler
import datetime
import numpy as np
import os
import sys
import time
import torch
import torch.distributed as dist
import torch.nn as nn

def has_batchnorms(model):
    bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.SyncBatchNorm)
    for name, module in model.named_modules():
        if isinstance(module, bn_types):
            return True
    return False

def get_params_groups(model):
    regularized = []
    not_regularized = []
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
        # we do not regularize biases nor Norm parameters
        if name.endswith(".bias") or len(param.shape) == 1:
            not_regularized.append(param)
        else:
            regularized.append(param)
    return [{'params': regularized}, {'params': not_regularized, 'weight_decay': 0.}]

def init_distributed_mode(args):
    # launched with torch.distributed.launch
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        args.rank = int(os.environ["RANK"])
        args.world_size = int(os.environ['WORLD_SIZE'])
        args.gpu = int(os.environ['LOCAL_RANK'])
    # launched with submitit on a slurm cluster
    elif 'SLURM_PROCID' in os.environ:
        args.rank = int(os.environ['SLURM_PROCID'])
        args.gpu = args.rank % torch.cuda.device_count()
    # launched naively with `python main_dino.py`
    # we manually add MASTER_ADDR and MASTER_PORT to env variables
    elif torch.cuda.is_available():
        print('Will run the code on one GPU.')
        args.rank, args.gpu, args.world_size = 0, 0, 1
        os.environ['MASTER_ADDR'] = '127.0.0.1'
        os.environ['MASTER_PORT'] = '29500'
    else:
        print('Does not support training without GPU.')
        sys.exit(1)

    dist.init_process_group(
        backend="nccl",
        init_method=args.dist_url,
        world_size=args.world_size,
        rank=args.rank,
    )

    torch.cuda.set_device(args.gpu)
    print('| distributed init (rank {}): {}'.format(
        args.rank, args.dist_url), flush=True)
    dist.barrier()
    setup_for_distributed(args.rank == 0)


def setup_for_distributed(is_master):
    """
    This function disables printing when not in master process
    """
    import builtins as __builtin__
    builtin_print = __builtin__.print

    def print(*args, **kwargs):
        force = kwargs.pop('force', False)
        if is_master or force:
            builtin_print(*args, **kwargs)

    __builtin__.print = print


def fix_random_seeds(seed=3407):
    """
    Fix random seeds.
    """
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)

def is_dist_avail_and_initialized():
    if not dist.is_available():
        return False
    if not dist.is_initialized():
        return False
    return True

def get_rank():
    if not is_dist_avail_and_initialized():
        return 0
    return dist.get_rank()

def is_main_process():
    return get_rank() == 0


def save_on_master(*args, **kwargs):
    if is_main_process():
        torch.save(*args, **kwargs)


class SmoothedValue(object):
    """Track a series of values and provide access to smoothed values over a
    window or the global series average.
    """

    def __init__(self, window_size=50, fmt=None, avg=False):
        if fmt is None:
            if avg:
                fmt = "{median:.6f} ({avg:.6f})"
            else:
                fmt = "{median:.6f} ({global_avg:.6f})"
        self.deque = deque(maxlen=window_size)
        self.total = 0.0
        self.count = 0
        self.fmt = fmt

    def update(self, value, n=1):
        self.deque.append(value)
        self.count += n
        self.total += value * n

    def synchronize_between_processes(self):
        """
        Warning: does not synchronize the deque!
        """
        if not is_dist_avail_and_initialized():
            return
        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')
        dist.barrier()
        dist.all_reduce(t)
        t = t.tolist()
        self.count = int(t[0])
        self.total = t[1]

    @property
    def median(self):
        d = torch.tensor(list(self.deque))
        return d.median().item()

    @property
    def avg(self):
        d = torch.tensor(list(self.deque), dtype=torch.float32)
        return d.mean().item()

    @property
    def global_avg(self):
        return self.total / self.count

    @property
    def max(self):
        return max(self.deque)

    @property
    def value(self):
        return self.deque[-1]

    def __str__(self):
        return self.fmt.format(
            median=self.median,
            avg=self.avg,
            global_avg=self.global_avg,
            max=self.max,
            value=self.value)

class MetricLogger(object):
    def __init__(self, delimiter="\t", avg=False):
        
        self.meters = defaultdict(lambda: SmoothedValue(avg=avg))
        self.delimiter = delimiter

    def update(self, **kwargs):
        for k, v in kwargs.items():
            if isinstance(v, torch.Tensor):
                v = v.item()
            assert isinstance(v, (float, int))
            self.meters[k].update(v)

    def __getattr__(self, attr):
        if attr in self.meters:
            return self.meters[attr]
        if attr in self.__dict__:
            return self.__dict__[attr]
        raise AttributeError("'{}' object has no attribute '{}'".format(
            type(self).__name__, attr))

    def __str__(self):
        loss_str = []
        for name, meter in self.meters.items():
            loss_str.append(
                "{}: {}".format(name, str(meter))
            )
        return self.delimiter.join(loss_str)

    def synchronize_between_processes(self):
        for meter in self.meters.values():
            meter.synchronize_between_processes()

    def add_meter(self, name, meter):
        self.meters[name] = meter

    def log_every(self, iterable, print_freq, header=None):
        i = 0
        if not header:
            header = ''
        start_time = time.time()
        end = time.time()
        iter_time = SmoothedValue(fmt='{avg:.6f}')
        data_time = SmoothedValue(fmt='{avg:.6f}')
        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'
        if torch.cuda.is_available():
            log_msg = self.delimiter.join([
                header,
                '[{0' + space_fmt + '}/{1}]',
                'eta: {eta}',
                '{meters}',
                'time: {time}',
                'data: {data}',
                'max mem: {memory:.0f}'
            ])
        else:
            log_msg = self.delimiter.join([
                header,
                '[{0' + space_fmt + '}/{1}]',
                'eta: {eta}',
                '{meters}',
                'time: {time}',
                'data: {data}'
            ])
        MB = 1024.0 * 1024.0
        for obj in iterable:
            data_time.update(time.time() - end)
            yield obj
            iter_time.update(time.time() - end)
            if i % print_freq == 0 or i == len(iterable) - 1:
                eta_seconds = iter_time.global_avg * (len(iterable) - i)
                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))
                if torch.cuda.is_available():
                    print(log_msg.format(
                        i, len(iterable), eta=eta_string,
                        meters=str(self),
                        time=str(iter_time), data=str(data_time),
                        memory=torch.cuda.max_memory_allocated() / MB))
                else:
                    print(log_msg.format(
                        i, len(iterable), eta=eta_string,
                        meters=str(self),
                        time=str(iter_time), data=str(data_time)))
            i += 1
            end = time.time()
        total_time = time.time() - start_time
        total_time_str = str(datetime.timedelta(seconds=int(total_time)))
        print('{} Total time: {} ({:.6f} s / it)'.format(
            header, total_time_str, total_time / len(iterable)))


def binary_acc(y_pred, y_true):
    y_pred = torch.round(torch.sigmoid(y_pred))

    correct_results_sum = (y_pred == y_true).sum().float()
    acc = correct_results_sum/y_true.shape[0]
    
    acc = torch.round(acc * 100)
    return acc

def multi_acc(y_pred, y_true):
    y_pred = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)
    correct_results_sum = (y_pred == y_true).sum().float()
    acc = correct_results_sum/y_true.shape[0]
    acc = torch.round(acc * 100)
    return acc

# https://github.com/pytorch/pytorch/issues/7359
class WeightedSubsetRandomSampler(Sampler):
    r"""Samples elements from a given list of indices with given probabilities (weights), with replacement.

    Arguments:
        weights (sequence)   : a sequence of weights, not necessary summing up to one
        num_samples (int): number of samples to draw
    """

    def __init__(self, subset_indices, fullset_labels):
        self.subset_indices = subset_indices
        self.num_samples = len(self.subset_indices)

        _, counts = np.unique(fullset_labels, return_counts=True)
        self.weights = 1. / torch.tensor(counts, dtype=torch.float)

        print('weighted subset sampler initiated; weights: ', self.weights)

        samples_weight_fullset = np.array([self.weights[t] for t in fullset_labels])
        samples_weight_subset = np.array([samples_weight_fullset[i] for i in self.subset_indices])
        self.samples_weight = torch.tensor(samples_weight_subset, dtype=torch.double)

    def __iter__(self):
        return (self.subset_indices[i] for i in torch.multinomial(self.samples_weight, self.num_samples))

    def __len__(self):
        return len(self.subset_indices)

    
    
# class DistributedSamplerWrapper(DistributedSampler):
#     """
#     Wrapper over `Sampler` for distributed training.
#     Allows you to use any sampler in distributed mode.
#     It is especially useful in conjunction with
#     `torch.nn.parallel.DistributedDataParallel`. In such case, each
#     process can pass a DistributedSamplerWrapper instance as a DataLoader
#     sampler, and load a subset of subsampled data of the original dataset
#     that is exclusive to it.
#     .. note::
#         Sampler is assumed to be of constant size.
#     """

#     def __init__(
#         self,
#         sampler,
#         num_replicas = None,
#         rank = None,
#         shuffle: bool = True,
#     ):
#         """
#         Args:
#             sampler: Sampler used for subsampling
#             num_replicas (int, optional): Number of processes participating in
#               distributed training
#             rank (int, optional): Rank of the current process
#               within ``num_replicas``
#             shuffle (bool, optional): If true (default),
#               sampler will shuffle the indices
#         """
#         super(DistributedSamplerWrapper, self).__init__(
#             DatasetFromSampler(sampler),
#             num_replicas=num_replicas,
#             rank=rank,
#             shuffle=shuffle,
#         )
#         self.sampler = sampler

#     def update_weights(self, epoch):
#         self.sampler.update_weights(epoch)

#     def __iter__(self):
#         """@TODO: Docs. Contribution is welcome."""
#         self.dataset = DatasetFromSampler(self.sampler)
#         indexes_of_indexes = super().__iter__()
#         subsampler_indexes = self.dataset
#         return iter(itemgetter(*indexes_of_indexes)(subsampler_indexes))
    
    
def GetHumanReadable(size,precision=2):
    suffixes=['B','KB','MB','GB','TB']
    suffixIndex = 0
    while size > 1024 and suffixIndex < 4:
        suffixIndex += 1 #increment the index of the suffix
        size = size/1024.0 #apply the division
    return "%.*f%s"%(precision,size,suffixes[suffixIndex])


class DistributedEvalSampler(Sampler):
    r"""
    DistributedEvalSampler is different from DistributedSampler.
    It does NOT add extra samples to make it evenly divisible.
    DistributedEvalSampler should NOT be used for training. The distributed processes could hang forever.
    See this issue for details: https://github.com/pytorch/pytorch/issues/22584
    shuffle is disabled by default

    DistributedEvalSampler is for evaluation purpose where synchronization does not happen every epoch.
    Synchronization should be done outside the dataloader loop.

    Sampler that restricts data loading to a subset of the dataset.

    It is especially useful in conjunction with
    :class:`torch.nn.parallel.DistributedDataParallel`. In such a case, each
    process can pass a :class`~torch.utils.data.DistributedSampler` instance as a
    :class:`~torch.utils.data.DataLoader` sampler, and load a subset of the
    original dataset that is exclusive to it.

    .. note::
        Dataset is assumed to be of constant size.

    Arguments:
        dataset: Dataset used for sampling.
        num_replicas (int, optional): Number of processes participating in
            distributed training. By default, :attr:`rank` is retrieved from the
            current distributed group.
        rank (int, optional): Rank of the current process within :attr:`num_replicas`.
            By default, :attr:`rank` is retrieved from the current distributed
            group.
        shuffle (bool, optional): If ``True`` (default), sampler will shuffle the
            indices.
        seed (int, optional): random seed used to shuffle the sampler if
            :attr:`shuffle=True`. This number should be identical across all
            processes in the distributed group. Default: ``0``.

    .. warning::
        In distributed mode, calling the :meth`set_epoch(epoch) <set_epoch>` method at
        the beginning of each epoch **before** creating the :class:`DataLoader` iterator
        is necessary to make shuffling work properly across multiple epochs. Otherwise,
        the same ordering will be always used.

    Example::

        >>> sampler = DistributedSampler(dataset) if is_distributed else None
        >>> loader = DataLoader(dataset, shuffle=(sampler is None),
        ...                     sampler=sampler)
        >>> for epoch in range(start_epoch, n_epochs):
        ...     if is_distributed:
        ...         sampler.set_epoch(epoch)
        ...     train(loader)
    """

    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=False, seed=0):
        if num_replicas is None:
            if not dist.is_available():
                raise RuntimeError("Requires distributed package to be available")
            num_replicas = dist.get_world_size()
        if rank is None:
            if not dist.is_available():
                raise RuntimeError("Requires distributed package to be available")
            rank = dist.get_rank()
        self.dataset = dataset
        self.num_replicas = num_replicas
        self.rank = rank
        self.epoch = 0
        # self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))
        # self.total_size = self.num_samples * self.num_replicas
        self.total_size = len(self.dataset)         # true value without extra samples
        indices = list(range(self.total_size))
        indices = indices[self.rank:self.total_size:self.num_replicas]
        self.num_samples = len(indices)             # true value without extra samples

        self.shuffle = shuffle
        self.seed = seed

    def __iter__(self):
        if self.shuffle:
            # deterministically shuffle based on epoch and seed
            g = torch.Generator()
            g.manual_seed(self.seed + self.epoch)
            indices = torch.randperm(len(self.dataset), generator=g).tolist()
        else:
            indices = list(range(len(self.dataset)))


        # # add extra samples to make it evenly divisible
        # indices += indices[:(self.total_size - len(indices))]
        # assert len(indices) == self.total_size

        # subsample
        indices = indices[self.rank:self.total_size:self.num_replicas]
        assert len(indices) == self.num_samples

        return iter(indices)

    def __len__(self):
        return self.num_samples

    def set_epoch(self, epoch):
        r"""
        Sets the epoch for this sampler. When :attr:`shuffle=True`, this ensures all replicas
        use a different random ordering for each epoch. Otherwise, the next iteration of this
        sampler will yield the same ordering.

        Arguments:
            epoch (int): _epoch number.
        """
        self.epoch = epoch

def find_pth(backbone_name):
    sc = f'./**/ckpt_bestsofar_*_{backbone_name}.pth'
    av = glob(sc, recursive=True)
    if len(av) > 1:
        print(f'WARNING: more than one .pth output files found matching "ckpt_bestsofar_*date*_{backbone_name}.pth"')
        print('BY DEFAULT, USING: ', av[0])
        print("please rename or move the files that you do no wish to use and try again")
    elif len(av) == 0:
        print(f'WARNING: NO .pth output files found matching "ckpt_bestsofar_*date*_{backbone_name}.pth"')
        raise ImportError(f'Training the Siamese Ensemble requires to have already trained each subnetwork individually first')
    return av[0]

# naming conventions; (changed modules names after already trained sme models...)
def load_state_dict_partial(model, state_dict):
    model_state_dict = model.state_dict()
    loaded_keys, skipped_keys = [], []

    for key, value in state_dict.items():
        modified_key = key
        if "backbone.convnext_tiny" in key:
            modified_key = key.replace("backbone.convnext_tiny", "backbone.net")
        if "backbone.efficientnet_v2_s" in key:
            modified_key = key.replace("backbone.efficientnet_v2_s", "backbone.net")
        if "backbone.inception_resnet_v2" in key:
            modified_key = key.replace("backbone.inception_resnet_v2", "backbone.net")
        elif "model1.backbone.convnext_tiny" in key:
            modified_key = key.replace("model1.backbone.convnext_tiny", "model1.backbone.net")
        elif "model2.backbone.efficientnet_v2_s" in key:
            modified_key = key.replace("model2.backbone.efficientnet_v2_s", "model2.backbone.net")
        elif "model3.backbone.inception_resnet_v2" in key:
            modified_key = key.replace("model3.backbone.inception_resnet_v2", "model3.backbone.net")
        
        if modified_key in model_state_dict and model_state_dict[modified_key].shape == value.shape:
            model_state_dict[modified_key] = value
            loaded_keys.append(modified_key)
        else:
            skipped_keys.append(key)
    
    model.load_state_dict(model_state_dict, strict=True)
    # print(f"Loaded keys: {loaded_keys}")
    print(f"Skipped keys: {skipped_keys}")